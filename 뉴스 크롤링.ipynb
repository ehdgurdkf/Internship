{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = 'D:/python study/beautifulSoup_ws/crawling_result/' #결과물을 저장할 공간\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(n_url):\n",
    "    news_detail = []\n",
    "\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "\n",
    "    title = bsoup.select('h3#articleTitle')[0].text  #대괄호는  h3#articleTitle 인 것중 첫번째 그룹만 가져오겠다.\n",
    "    news_detail.append(title)\n",
    "\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate)\n",
    "\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip())\n",
    "  \n",
    "    news_detail.append(n_url)\n",
    "    \n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany)\n",
    "\n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(maxpage,query,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    f = open(\"D:/python study/beautifulSoup_ws/crawling_result/contents_text.txt\", 'w', encoding='utf-8')\n",
    "    \n",
    "    while page < maxpage_t:\n",
    "    \n",
    "        print(page)\n",
    "    \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "            #print(soup)\n",
    "    \n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                #print(urls[\"href\"])\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    #print(urls[\"href\"])\n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "                        # pdate, pcompany, title, btext\n",
    "                    f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[4], news_detail[0], news_detail[2],news_detail[3]))  # new style\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_make():\n",
    "    data = pd.read_csv(RESULT_PATH+'contents_text.txt', sep='\\t',header=None, error_bad_lines=False)\n",
    "    data.columns = ['years','company','title','contents','link']\n",
    "    print(data)\n",
    "    \n",
    "    xlsx_outputFileName = '%s-%s-%s  %s시 %s분 %s초 result.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    #xlsx_name = 'result' + '.xlsx'\n",
    "    data.to_excel(RESULT_PATH+xlsx_outputFileName, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 출력할 페이지수 입력하시오: 3\n",
      "검색어 입력: 충북 복숭아 강수\n",
      "시작날짜 입력, ex) 2019.01.01:2014.01.01\n",
      "끝날짜 입력 , ex) 2020.05.26:2019.12.31\n",
      "1\n",
      "https://search.naver.com/search.naver?where=news&query=충북 복숭아 강수&sort=0&ds=2014.01.01&de=2019.12.31&nso=so%3Ar%2Cp%3Afrom20140101to20191231%2Ca%3A&start=1\n",
      "11\n",
      "https://search.naver.com/search.naver?where=news&query=충북 복숭아 강수&sort=0&ds=2014.01.01&de=2019.12.31&nso=so%3Ar%2Cp%3Afrom20140101to20191231%2Ca%3A&start=11\n",
      "         years         company                                         title  \\\n",
      "0  2019.07.28.            연합뉴스                     오락가락 장맛비에도 충북 국립공원·유원지 붐벼   \n",
      "1  2014.07.06.  financial news                    롯데마트 “자두 복숭아 여름 대표 과일로 부상”   \n",
      "2  2014.06.30.            연합뉴스                      늦서리·우박·가뭄·병해충…충북 농촌 '비상'   \n",
      "3  2015.06.20.            연합뉴스                 단비에도 해갈 역부족…충북 가뭄피해 농작물 1천61㏊   \n",
      "4  2018.03.29.           정책브리핑              배·복숭아 꽃 피는 시기, 평년과 비슷하거나 조금 빠를 듯   \n",
      "5  2016.07.06.            노컷뉴스               충북 비 피해 속출…농경지 52.1ha 침수·오리 떼죽음   \n",
      "6  2017.08.17.            한경닷컴  [한경·네이버 FARM] \"복숭아 맛있게 먹는 비법? 냉장고에 보관하지 마세요\"   \n",
      "\n",
      "                                            contents  \\\n",
      "0  (청주=연합뉴스) 윤우용 기자 = 장맛비가 오락가락하는 궂은 날씨를 보인 28일 충...   \n",
      "1  자두와 복숭아가 여름 대표 과일로 부상하고 있다.롯데마트는 지난달 국산과일 판매 동...   \n",
      "2  충주시 멸강나방 방제      (충주=연합뉴스) 노승혁 기자 = 30일 충북 충주시...   \n",
      "3  말라버린 마늘밭<<연합뉴스 DB>> 밭작물 피해 11일 893㏊→20일 1천6㏊, ...   \n",
      "4  - 이상기상에 대비하고 인공수분 준비 철저히 해야 -농촌진흥청(청장 라승용)은 올해...   \n",
      "5  [청주CBS 장나래 기자]침수된 오리농장(사진=영동군 제공)충북에서도 엿새 동안 장...   \n",
      "6  명인의 맛있는 복숭아 비법김종오 해돋이농원 대표복숭아는 여름 대표 과일 중 하나다....   \n",
      "\n",
      "                                                link  \n",
      "0  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "1  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "2  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "3  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "4  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "5  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "6  https://news.naver.com/main/read.nhn?mode=LSD&...  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    maxpage = input(\"최대 출력할 페이지수 입력하시오: \") \n",
    "    query = input(\"검색어 입력: \")\n",
    "    s_date = input(\"시작날짜 입력, ex) 2019.01.01:\")  #2019.01.01\n",
    "    e_date = input(\"끝날짜 입력 , ex) 2020.05.26:\")   #2019.04.28\n",
    "    crawler(maxpage,query,s_date,e_date) #검색된 네이버뉴스의 기사내용을 크롤링합니다. \n",
    "    \n",
    "    excel_make() #엑셀로 만들기 \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
